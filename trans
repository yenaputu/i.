from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# Set device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Device: {device}")

# Load tokenizer & model
model_id = "HuggingFaceH4/zephyr-7b-beta"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if device=="cuda" else torch.float32)
model.to(device)

# Pipeline untuk text generation
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device=0 if device == "cuda" else -1)

# Format prompt untuk chat
def format_prompt(user_input):
    return f"<|system|>\nKamu adalah W.E.D.I, asisten AI yang ramah.\n<|user|>\n{user_input}\n<|assistant|>"

# Fungsi AI
class WEDI:
    def __init__(self):
        self.history = []

    def ask(self, user_input):
        prompt = format_prompt(user_input)
        result = pipe(prompt, max_new_tokens=200, do_sample=True, temperature=0.7)[0]["generated_text"]
        response = result.split("<|assistant|>")[-1].strip()
        return response

# Program utama
if __name__ == "__main__":
    ai = WEDI()
    print("Halo! Saya W.E.D.I, asisten cerdas Anda. Tanyakan apa saja.")
    while True:
        user_input = input("Anda: ")
        if user_input.lower() in ["exit", "keluar"]:
            break
        response = ai.ask(user_input)
        print("W.E.D.I:", response)